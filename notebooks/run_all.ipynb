{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb920ffe-395c-4281-9bf3-b64d1349dc04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/code/mmbu/inference/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 16:39:32 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from run_vlm_eval import main, load_config, set_envs, log_first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdfc3a5-151e-417b-a7fb-e5043224d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vqa_dataset import PromptDataset, prompt_collate, create_template\n",
    "from models import load_model_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae90eb8-ecba-4376-9680-2c29e58d3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/test_config.yaml\")\n",
    "model_cfg = cfg[\"model\"]\n",
    "tasks_cfg = cfg[\"tasks\"]\n",
    "run_cfg  = cfg[\"runtime\"]\n",
    "output_dir = '/pasteur/u/rdcunha/code/mmbu/results'\n",
    "\n",
    "model_type = model_cfg[\"type\"]\n",
    "model_name = model_cfg[\"name\"]\n",
    "device     = model_cfg.get(\"device\", \"auto\")\n",
    "cache_dir  = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "set_envs(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba58b0b-9d44-4ded-b68a-fe8ee2b005e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ace48405a24f28b625bc05b136a74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "adapter = load_model_adapter(model_type, model_name, device, cache_dir)\n",
    "model, processor = adapter.load()\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_model_name = model_name.split('/')[-1]\n",
    "model_path = file_model_name.replace('/', '_')\n",
    "output_dir = os.path.join(output_dir, model_path)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0769cc-2b58-45be-8d48-5431d75e0205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running task: detection_grounding_open_VQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 3128/3128 [00:53<00:00, 58.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All images loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  65%|█▎| 204/313 [00:04<00:01, 61.04it/s]"
     ]
    }
   ],
   "source": [
    "base_path = '/pasteur/u/rdcunha/data_cache/mmbu/final_data/subsampled_mmbu_data'\n",
    "\n",
    "for task_cfg in tasks_cfg:\n",
    "    print(f\"Running task: {task_cfg['name']}\")\n",
    "    out_file = os.path.join(output_dir, f\"{file_model_name.replace('/', '_')}_{task_cfg['name']}.jsonl\")\n",
    "    tsv_path = os.path.join(base_path, task_cfg[\"data_path\"])\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    add_options = (\"open\" not in task_cfg[\"name\"])\n",
    "    dataset = PromptDataset(df=df, add_options=add_options)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=run_cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=prompt_collate,\n",
    "        num_workers=4,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=4\n",
    "    )\n",
    "\n",
    "    existing = set()\n",
    "    if os.path.exists(out_file):\n",
    "        with open(out_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    j = json.loads(line)\n",
    "                    existing.add(j[\"index\"])\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "    counter = 0\n",
    "    saved = []\n",
    "    first_batch_logged = False\n",
    "    \n",
    "    with open(out_file, \"a\") as f:\n",
    "        for batch in tqdm(loader, desc=\"Inference\"):\n",
    "    \n",
    "            new_batch = [x for x in batch if x[\"index\"] not in existing]\n",
    "            if not new_batch:\n",
    "                continue\n",
    "    \n",
    "            # inference\n",
    "            # try:\n",
    "                # messages = [create_template(item) for item in new_batch]\n",
    "            messages = [adapter.create_template(item) for item in new_batch]\n",
    "            # model-specific input prep\n",
    "            inputs = adapter.prepare_inputs(messages, processor, model)\n",
    "            outputs = adapter.infer(model, processor, inputs, run_cfg[\"max_new_tokens\"])\n",
    "            # except: \n",
    "            #     print(f\"could not generate for {batch}\")\n",
    "            #     continue\n",
    "    \n",
    "            # log first batch only\n",
    "            if run_cfg[\"log_first_batch\"] and not first_batch_logged:\n",
    "                log_first_batch(outputs, output_dir)\n",
    "                first_batch_logged = True\n",
    "    \n",
    "            # save results\n",
    "            for it, out_text in zip(new_batch, outputs):\n",
    "                obj = {\n",
    "                    \"index\": it[\"index\"],\n",
    "                    \"question\": it[\"question\"],\n",
    "                    \"image_path\": it[\"image_path\"],\n",
    "                    \"dataset\": it[\"dataset\"],\n",
    "                    \"modality\": it[\"modality\"],\n",
    "                    \"class_label\": it[\"class_label\"],\n",
    "                    \"answer\": out_text\n",
    "                }\n",
    "                if \"options\" in it and it[\"options\"] is not None:\n",
    "                    obj[\"options\"] = it[\"options\"]\n",
    "            \n",
    "                saved.append(obj)\n",
    "                existing.add(it[\"index\"])\n",
    "                counter += 1\n",
    "    \n",
    "                if counter % 50 == 0:\n",
    "                    for s in saved:\n",
    "                        f.write(json.dumps(s) + \"\\n\")\n",
    "                    f.flush()\n",
    "                    saved = []\n",
    "    \n",
    "        # Save remainder\n",
    "        for s in saved:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd637601-3ebf-4414-afc8-fdc17917acd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PIL:\n",
      "PIL loaded successfully\n",
      "\n",
      "Testing OpenCV:\n",
      "OpenCV loaded: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng error: IDAT: CRC error\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import cv2\n",
    "\n",
    "path = \"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/standarized-subsampled/extra_det_v2/malaria-bounding-boxes/det/images_with_bbox/8757be1e-b832-407a-8e95-62abae485b24__bbox.png\"\n",
    "\n",
    "print(\"Testing PIL:\")\n",
    "try:\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    img = Image.open(path)\n",
    "    img.load()\n",
    "    print(\"PIL loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(\"PIL error:\", e)\n",
    "\n",
    "print(\"\\nTesting OpenCV:\")\n",
    "img_cv = cv2.imread(path)\n",
    "print(\"OpenCV loaded:\", img_cv is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcb9a9-1c54-4639-be65-abd45fe48889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
