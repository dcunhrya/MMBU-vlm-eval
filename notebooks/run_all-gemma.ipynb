{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb920ffe-395c-4281-9bf3-b64d1349dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/code/mmbu/inference/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 16:28:36 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from run_vlm_eval import main, load_config, set_envs, log_first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdfc3a5-151e-417b-a7fb-e5043224d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vqa_dataset import PromptDataset, prompt_collate, create_template\n",
    "from models import load_model_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae90eb8-ecba-4376-9680-2c29e58d3ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'configs/test_config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cfg = \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigs/test_config.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model_cfg = cfg[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m tasks_cfg = cfg[\u001b[33m\"\u001b[39m\u001b[33mtasks\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/code/mmbu/inference/src/run_vlm_eval.py:16\u001b[39m, in \u001b[36mload_config\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_config\u001b[39m(path):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     17\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m yaml.safe_load(f)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'configs/test_config.yaml'"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"configs/test_config.yaml\")\n",
    "model_cfg = cfg[\"model\"]\n",
    "tasks_cfg = cfg[\"tasks\"]\n",
    "run_cfg  = cfg[\"runtime\"]\n",
    "output_dir = '/pasteur/u/rdcunha/code/mmbu/results'\n",
    "\n",
    "model_type = model_cfg[\"type\"]\n",
    "model_name = model_cfg[\"name\"]\n",
    "device     = model_cfg.get(\"device\", \"auto\")\n",
    "cache_dir  = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "set_envs(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85acf242-67cd-4d99-8659-28b78b61720a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d61f1a4584941c5b3598d97572bafbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "adapter = load_model_adapter(model_type, model_name, device, cache_dir)\n",
    "model, processor = adapter.load()\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_model_name = model_name.split('/')[-1]\n",
    "model_path = file_model_name.replace('/', '_')\n",
    "output_dir = os.path.join(output_dir, model_path)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Track processed items\n",
    "# existing = set()\n",
    "# if os.path.exists(out_file):\n",
    "#     with open(out_file, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             try:\n",
    "#                 j = json.loads(line)\n",
    "#                 existing.add(j[\"index\"])\n",
    "#             except:\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f0769cc-2b58-45be-8d48-5431d75e0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running task: detection_grounding_open_VQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 3128/3128 [00:06<00:00, 481.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All images loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  17%|███▊                   | 52/313 [00:00<00:03, 84.29it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Inference:  77%|█████████████████     | 242/313 [03:19<07:53,  6.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  87%|███████████████████   | 271/313 [07:37<06:20,  9.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  87%|███████████████████   | 272/313 [07:47<06:23,  9.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  87%|███████████████████▏  | 273/313 [07:58<06:37,  9.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  88%|███████████████████▎  | 274/313 [08:12<07:17, 11.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  88%|███████████████████▎  | 275/313 [08:25<07:21, 11.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  88%|███████████████████▍  | 276/313 [08:36<07:10, 11.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  88%|███████████████████▍  | 277/313 [08:46<06:32, 10.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  89%|███████████████████▌  | 278/313 [08:58<06:42, 11.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  89%|███████████████████▌  | 279/313 [09:11<06:40, 11.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  89%|███████████████████▋  | 280/313 [09:20<06:07, 11.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  90%|███████████████████▊  | 281/313 [09:29<05:31, 10.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  90%|███████████████████▊  | 282/313 [09:38<05:11, 10.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  90%|███████████████████▉  | 283/313 [10:41<12:50, 25.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  91%|███████████████████▉  | 284/313 [10:52<10:24, 21.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  91%|████████████████████  | 285/313 [11:05<08:50, 18.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  91%|████████████████████  | 286/313 [11:15<07:18, 16.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  92%|████████████████████▏ | 287/313 [11:25<06:08, 14.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  92%|████████████████████▏ | 288/313 [11:34<05:17, 12.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  92%|████████████████████▎ | 289/313 [12:36<11:00, 27.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  93%|████████████████████▍ | 290/313 [12:46<08:33, 22.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  93%|████████████████████▍ | 291/313 [12:59<07:09, 19.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  93%|████████████████████▌ | 292/313 [13:48<09:57, 28.44s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  94%|████████████████████▌ | 293/313 [14:19<09:39, 28.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  94%|████████████████████▋ | 294/313 [14:41<08:34, 27.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  94%|████████████████████▋ | 295/313 [15:28<09:51, 32.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  95%|████████████████████▊ | 296/313 [15:50<08:24, 29.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  95%|████████████████████▉ | 297/313 [16:12<07:17, 27.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  95%|████████████████████▉ | 298/313 [16:21<05:28, 21.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  96%|█████████████████████ | 299/313 [16:36<04:39, 19.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  96%|█████████████████████ | 300/313 [16:50<03:54, 18.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  96%|█████████████████████▏| 301/313 [17:52<06:15, 31.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  96%|█████████████████████▏| 302/313 [18:02<04:33, 24.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  97%|█████████████████████▎| 303/313 [18:12<03:24, 20.48s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  97%|█████████████████████▎| 304/313 [18:22<02:35, 17.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  97%|█████████████████████▍| 305/313 [18:32<02:00, 15.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  98%|█████████████████████▌| 306/313 [19:34<03:24, 29.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  98%|█████████████████████▌| 307/313 [19:44<02:20, 23.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  98%|█████████████████████▋| 308/313 [19:54<01:37, 19.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  99%|█████████████████████▋| 309/313 [20:00<01:01, 15.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  99%|█████████████████████▊| 310/313 [20:07<00:38, 12.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference:  99%|█████████████████████▊| 311/313 [20:13<00:21, 10.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference: 100%|█████████████████████▉| 312/313 [21:16<00:26, 26.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Inference: 100%|██████████████████████| 313/313 [22:12<00:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = '/pasteur/u/rdcunha/data_cache/mmbu/final_data/subsampled_mmbu_data'\n",
    "\n",
    "for task_cfg in tasks_cfg:\n",
    "    print(f'Running task: {task_cfg['name']}')\n",
    "    out_file = os.path.join(output_dir, f\"{file_model_name.replace('/', '_')}_{task_cfg['name']}.jsonl\")\n",
    "    tsv_path = os.path.join(base_path, task_cfg[\"data_path\"])\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    add_options = (\"open\" not in task_cfg[\"name\"])\n",
    "    dataset = PromptDataset(df=df, add_options=add_options)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=run_cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=prompt_collate,\n",
    "        num_workers=4,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=4\n",
    "    )\n",
    "\n",
    "    existing = set()\n",
    "    if os.path.exists(out_file):\n",
    "        with open(out_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    j = json.loads(line)\n",
    "                    existing.add(j[\"index\"])\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "    counter = 0\n",
    "    saved = []\n",
    "    first_batch_logged = False\n",
    "    \n",
    "    with open(out_file, \"a\") as f:\n",
    "        for batch in tqdm(loader, desc=\"Inference\"):\n",
    "    \n",
    "            new_batch = [x for x in batch if x[\"index\"] not in existing]\n",
    "            if not new_batch:\n",
    "                continue\n",
    "    \n",
    "            # inference\n",
    "            try:\n",
    "                all_inputs = []\n",
    "                for item in new_batch:\n",
    "                    single_msg = adapter.create_template(item)\n",
    "                    single_inp = adapter.prepare_inputs([single_msg], processor, model)\n",
    "                    all_inputs.append(single_inp)\n",
    "                \n",
    "                batched_inputs = adapter.stack_inputs(all_inputs, model)\n",
    "                \n",
    "                outputs = adapter.infer(model, processor, batched_inputs, run_cfg[\"max_new_tokens\"])\n",
    "            except: \n",
    "                print(f\"could not generate for {batch}\")\n",
    "                continue\n",
    "    \n",
    "            # log first batch only\n",
    "            if run_cfg[\"log_first_batch\"] and not first_batch_logged:\n",
    "                log_first_batch(outputs, output_dir)\n",
    "                first_batch_logged = True\n",
    "    \n",
    "            # save results\n",
    "            for it, out_text in zip(new_batch, outputs):\n",
    "                obj = {\n",
    "                    \"index\": it[\"index\"],\n",
    "                    \"question\": it[\"question\"],\n",
    "                    \"image_path\": it[\"image_path\"],\n",
    "                    \"dataset\": it[\"dataset\"],\n",
    "                    \"modality\": it[\"modality\"],\n",
    "                    \"class_label\": it[\"class_label\"],\n",
    "                    \"answer\": out_text\n",
    "                }\n",
    "                if \"options\" in it and it[\"options\"] is not None:\n",
    "                    obj[\"options\"] = it[\"options\"]\n",
    "            \n",
    "                saved.append(obj)\n",
    "                existing.add(it[\"index\"])\n",
    "                counter += 1\n",
    "    \n",
    "                if counter % 50 == 0:\n",
    "                    for s in saved:\n",
    "                        f.write(json.dumps(s) + \"\\n\")\n",
    "                    f.flush()\n",
    "                    saved = []\n",
    "    \n",
    "        # Save remainder\n",
    "        for s in saved:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239246f-f27d-446f-99d7-576138c60b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
