{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb920ffe-395c-4281-9bf3-b64d1349dc04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/code/mmbu/inference/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 07:09:13 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from run_vlm_eval import main, load_config, set_envs, log_first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdfc3a5-151e-417b-a7fb-e5043224d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vqa_dataset import PromptDataset, prompt_collate, create_template\n",
    "from models import load_model_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae90eb8-ecba-4376-9680-2c29e58d3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"configs/test_config.yaml\")\n",
    "model_cfg = cfg[\"model\"]\n",
    "tasks_cfg = cfg[\"tasks\"]\n",
    "run_cfg  = cfg[\"runtime\"]\n",
    "output_dir = '/pasteur/u/rdcunha/code/mmbu/results'\n",
    "\n",
    "model_type = model_cfg[\"type\"]\n",
    "model_name = model_cfg[\"name\"]\n",
    "device     = model_cfg.get(\"device\", \"auto\")\n",
    "cache_dir  = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "set_envs(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba58b0b-9d44-4ded-b68a-fe8ee2b005e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ed3f4f967747b89f14159a5836dec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "adapter = load_model_adapter(model_type, model_name, device, cache_dir)\n",
    "model, processor = adapter.load()\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_model_name = model_name.split('/')[-1]\n",
    "model_path = file_model_name.replace('/', '_')\n",
    "output_dir = os.path.join(output_dir, model_path)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f0769cc-2b58-45be-8d48-5431d75e0205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running task: detection_grounding_open_VQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 3128/3128 [00:09<00:00, 321.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All images loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|                            | 0/313 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Inference:   1%|▏                   | 3/313 [00:29<50:24,  9.76s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m messages = [adapter.create_template(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m new_batch]\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# model-specific input prep\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m inputs = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m outputs = adapter.infer(model, processor, inputs, run_cfg[\u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# except: \u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#     print(f\"could not generate for {batch}\")\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# log first batch only\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/code/mmbu/inference/models/gemma3.py:24\u001b[39m, in \u001b[36mGemma3Adapter.prepare_inputs\u001b[39m\u001b[34m(self, messages, processor, model)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages, processor, model):\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# messages is list[list[dict]] from build_messages()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     inputs = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.to(model.device, dtype=torch.bfloat16)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/code/mmbu/inference/.venv/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/code/mmbu/inference/.venv/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/code/mmbu/inference/.venv/lib/python3.13/site-packages/transformers/processing_utils.py:1705\u001b[39m, in \u001b[36mProcessorMixin.apply_chat_template\u001b[39m\u001b[34m(self, conversation, chat_template, **kwargs)\u001b[39m\n\u001b[32m   1703\u001b[39m images_exist = \u001b[38;5;28many\u001b[39m((im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m im_list \u001b[38;5;129;01min\u001b[39;00m batch_images \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m im_list)\n\u001b[32m   1704\u001b[39m videos_exist = \u001b[38;5;28many\u001b[39m((vid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m vid_list \u001b[38;5;129;01min\u001b[39;00m batch_videos \u001b[38;5;28;01mfor\u001b[39;00m vid \u001b[38;5;129;01min\u001b[39;00m vid_list)\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages_exist\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_videos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvideos_exist\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_audios\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_audios\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[32m   1714\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m processed_kwargs[\u001b[33m\"\u001b[39m\u001b[33mtemplate_kwargs\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mreturn_assistant_tokens_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/code/mmbu/inference/.venv/lib/python3.13/site-packages/transformers/models/gemma3/processing_gemma3.py:148\u001b[39m, in \u001b[36mGemma3Processor.__call__\u001b[39m\u001b[34m(self, images, text, videos, audio, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Add token type ids manually, as tokenizer can't do arbitrary position token types\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_mm_token_type_ids:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     array_ids = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     mm_token_type_ids = np.zeros_like(array_ids)\n\u001b[32m    150\u001b[39m     mm_token_type_ids[array_ids == \u001b[38;5;28mself\u001b[39m.image_token_id] = \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "base_path = '/pasteur/u/rdcunha/data_cache/mmbu/final_data/subsampled_mmbu_data'\n",
    "\n",
    "for task_cfg in tasks_cfg:\n",
    "    print(f\"Running task: {task_cfg['name']}\")\n",
    "    out_file = os.path.join(output_dir, f\"{file_model_name.replace('/', '_')}_{task_cfg['name']}.jsonl\")\n",
    "    tsv_path = os.path.join(base_path, task_cfg[\"data_path\"])\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    add_options = (\"open\" not in task_cfg[\"name\"])\n",
    "    dataset = PromptDataset(df=df, add_options=add_options)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=run_cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=prompt_collate,\n",
    "        num_workers=4,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=4\n",
    "    )\n",
    "\n",
    "    existing = set()\n",
    "    if os.path.exists(out_file):\n",
    "        with open(out_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    j = json.loads(line)\n",
    "                    existing.add(j[\"index\"])\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "    counter = 0\n",
    "    saved = []\n",
    "    first_batch_logged = False\n",
    "    \n",
    "    with open(out_file, \"a\") as f:\n",
    "        for batch in tqdm(loader, desc=\"Inference\"):\n",
    "    \n",
    "            new_batch = [x for x in batch if x[\"index\"] not in existing]\n",
    "            if not new_batch:\n",
    "                continue\n",
    "    \n",
    "            # inference\n",
    "            # try:\n",
    "                # messages = [create_template(item) for item in new_batch]\n",
    "            messages = [adapter.create_template(item) for item in new_batch]\n",
    "            # model-specific input prep\n",
    "            inputs = adapter.prepare_inputs(messages, processor, model)\n",
    "            outputs = adapter.infer(model, processor, inputs, run_cfg[\"max_new_tokens\"])\n",
    "            # except: \n",
    "            #     print(f\"could not generate for {batch}\")\n",
    "            #     continue\n",
    "    \n",
    "            # log first batch only\n",
    "            if run_cfg[\"log_first_batch\"] and not first_batch_logged:\n",
    "                log_first_batch(outputs, output_dir)\n",
    "                first_batch_logged = True\n",
    "    \n",
    "            # save results\n",
    "            for it, out_text in zip(new_batch, outputs):\n",
    "                obj = {\n",
    "                    \"index\": it[\"index\"],\n",
    "                    \"question\": it[\"question\"],\n",
    "                    \"image_path\": it[\"image_path\"],\n",
    "                    \"dataset\": it[\"dataset\"],\n",
    "                    \"modality\": it[\"modality\"],\n",
    "                    \"class_label\": it[\"class_label\"],\n",
    "                    \"answer\": out_text\n",
    "                }\n",
    "                if \"options\" in it and it[\"options\"] is not None:\n",
    "                    obj[\"options\"] = it[\"options\"]\n",
    "            \n",
    "                saved.append(obj)\n",
    "                existing.add(it[\"index\"])\n",
    "                counter += 1\n",
    "    \n",
    "                if counter % 50 == 0:\n",
    "                    for s in saved:\n",
    "                        f.write(json.dumps(s) + \"\\n\")\n",
    "                    f.flush()\n",
    "                    saved = []\n",
    "    \n",
    "        # Save remainder\n",
    "        for s in saved:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd637601-3ebf-4414-afc8-fdc17917acd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PIL:\n",
      "PIL loaded successfully\n",
      "\n",
      "Testing OpenCV:\n",
      "OpenCV loaded: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng error: IDAT: CRC error\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import cv2\n",
    "\n",
    "path = \"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/standarized-subsampled/extra_det_v2/malaria-bounding-boxes/det/images_with_bbox/8757be1e-b832-407a-8e95-62abae485b24__bbox.png\"\n",
    "\n",
    "print(\"Testing PIL:\")\n",
    "try:\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    img = Image.open(path)\n",
    "    img.load()\n",
    "    print(\"PIL loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(\"PIL error:\", e)\n",
    "\n",
    "print(\"\\nTesting OpenCV:\")\n",
    "img_cv = cv2.imread(path)\n",
    "print(\"OpenCV loaded:\", img_cv is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcb9a9-1c54-4639-be65-abd45fe48889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
